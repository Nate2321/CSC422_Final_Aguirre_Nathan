{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8aeca4",
   "metadata": {},
   "source": [
    "# Model Development - Plant Disease Classification\n",
    "\n",
    "**Objective**: Build, train, and evaluate a deep learning model for plant disease classification using transfer learning.\n",
    "\n",
    "**ML Lifecycle Stages**:\n",
    "1. Problem Definition & Requirements\n",
    "2. Data Preparation & Loading\n",
    "3. Model Architecture Design\n",
    "4. Training Strategy & Configuration\n",
    "5. Model Training & Monitoring\n",
    "6. Performance Evaluation\n",
    "7. Hyperparameter Tuning & Optimization\n",
    "8. Final Model Selection & Validation\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates the complete end-to-end machine learning workflow, from initial problem formulation through final model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b1af8",
   "metadata": {},
   "source": [
    "## 1. Problem Definition & Requirements\n",
    "\n",
    "### Business Problem\n",
    "Agricultural productivity is significantly impacted by plant diseases. Early and accurate detection can:\n",
    "- Reduce crop losses\n",
    "- Minimize pesticide usage\n",
    "- Enable targeted treatment\n",
    "- Improve yield quality\n",
    "\n",
    "### Technical Problem\n",
    "**Task**: Multi-class image classification  \n",
    "**Input**: RGB images of plant leaves (224Ã—224 pixels)  \n",
    "**Output**: Disease class prediction (16 classes)  \n",
    "**Success Metrics**: \n",
    "- Accuracy > 85% (target)\n",
    "- Balanced performance across classes (F1-score > 0.80)\n",
    "- Fast inference time (< 1 second per image)\n",
    "\n",
    "### Constraints\n",
    "- **Hardware**: CPU-only training (no GPU)\n",
    "- **Dataset**: PlantVillage dataset (~20K images)\n",
    "- **Deployment**: Must be lightweight enough for mobile/edge devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c676e",
   "metadata": {},
   "source": [
    "## 2. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac873b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Add project source to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1a6e9",
   "metadata": {},
   "source": [
    "## 3. Configuration Management\n",
    "\n",
    "Load project configuration from `config.yaml` for reproducibility and easy experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = Path.cwd().parent / 'config.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Display key configuration\n",
    "print(\"ðŸ“‹ Project Configuration:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {config['model']['architecture']}\")\n",
    "print(f\"Number of classes: {config['model']['num_classes']}\")\n",
    "print(f\"Image size: {config['data']['image_size']}\")\n",
    "print(f\"Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"Number of epochs: {config['training']['num_epochs']}\")\n",
    "print(f\"Optimizer: {config['training']['optimizer']}\")\n",
    "print(f\"Scheduler: {config['training']['scheduler']}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60fcea",
   "metadata": {},
   "source": [
    "## 4. Data Loading & Preparation\n",
    "\n",
    "### 4.1 Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997e3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check preprocessed data\n",
    "from preprocessing.data_loader import PlantDiseaseDataset\n",
    "\n",
    "train_dir = Path.cwd().parent / 'data' / 'processed' / 'train'\n",
    "val_dir = Path.cwd().parent / 'data' / 'processed' / 'val'\n",
    "test_dir = Path.cwd().parent / 'data' / 'processed' / 'test'\n",
    "\n",
    "# Count images in each split\n",
    "def count_images(data_dir):\n",
    "    total = 0\n",
    "    class_counts = {}\n",
    "    for class_dir in data_dir.iterdir():\n",
    "        if class_dir.is_dir() and not class_dir.name.startswith('.'):\n",
    "            count = len(list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.JPG')))\n",
    "            class_counts[class_dir.name] = count\n",
    "            total += count\n",
    "    return total, class_counts\n",
    "\n",
    "train_total, train_classes = count_images(train_dir)\n",
    "val_total, val_classes = count_images(val_dir)\n",
    "test_total, test_classes = count_images(test_dir)\n",
    "\n",
    "print(\"ðŸ“Š Dataset Split Statistics:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training set:   {train_total:,} images ({train_total/(train_total+val_total+test_total)*100:.1f}%)\")\n",
    "print(f\"Validation set: {val_total:,} images ({val_total/(train_total+val_total+test_total)*100:.1f}%)\")\n",
    "print(f\"Test set:       {test_total:,} images ({test_total/(train_total+val_total+test_total)*100:.1f}%)\")\n",
    "print(f\"Total:          {train_total+val_total+test_total:,} images\")\n",
    "print(f\"Number of classes: {len(train_classes)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884790b8",
   "metadata": {},
   "source": [
    "### 4.2 Data Augmentation Strategy\n",
    "\n",
    "Data augmentation is crucial for:\n",
    "1. **Preventing overfitting** - Increases effective training set size\n",
    "2. **Improving generalization** - Model learns invariant features\n",
    "3. **Handling variations** - Rotation, lighting, scale variations in real-world deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import augmentation module\n",
    "from preprocessing.augmentation import get_training_transforms, get_validation_transforms\n",
    "\n",
    "# Get transforms\n",
    "train_transform = get_training_transforms(\n",
    "    image_size=config['data']['image_size'],\n",
    "    advanced=True  # Use Albumentations for advanced augmentation\n",
    ")\n",
    "\n",
    "val_transform = get_validation_transforms(\n",
    "    image_size=config['data']['image_size']\n",
    ")\n",
    "\n",
    "print(\"ðŸ”„ Data Augmentation Pipeline:\")\n",
    "print(\"\\nTraining Augmentations:\")\n",
    "print(\"  âœ“ Random rotation (Â±20Â°)\")\n",
    "print(\"  âœ“ Horizontal flip (p=0.5)\")\n",
    "print(\"  âœ“ Vertical flip (p=0.3)\")\n",
    "print(\"  âœ“ Brightness/Contrast adjustment\")\n",
    "print(\"  âœ“ Gaussian blur (p=0.2)\")\n",
    "print(\"  âœ“ RGB shift\")\n",
    "print(\"  âœ“ Normalization (ImageNet stats)\")\n",
    "print(\"\\nValidation/Test:\")\n",
    "print(\"  âœ“ Resize to 224Ã—224\")\n",
    "print(\"  âœ“ Normalization only (no augmentation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a810a754",
   "metadata": {},
   "source": [
    "### 4.3 Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.data_loader import create_dataloaders\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader, test_loader, class_names = create_dataloaders(\n",
    "    train_dir=str(train_dir),\n",
    "    val_dir=str(val_dir),\n",
    "    test_dir=str(test_dir),\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    num_workers=0,  # Set to 0 for macOS compatibility\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform\n",
    ")\n",
    "\n",
    "print(\"âœ… DataLoaders created successfully!\")\n",
    "print(f\"\\nBatch configuration:\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print(f\"\\nClass names ({len(class_names)}):\")\n",
    "for i, name in enumerate(class_names, 1):\n",
    "    print(f\"  {i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63200041",
   "metadata": {},
   "source": [
    "## 5. Model Architecture Design\n",
    "\n",
    "### 5.1 Transfer Learning Strategy\n",
    "\n",
    "**Why Transfer Learning?**\n",
    "- Pre-trained models have learned general visual features from ImageNet (1M+ images)\n",
    "- Faster convergence with less data\n",
    "- Better generalization\n",
    "- Reduced training time\n",
    "\n",
    "**Approach:**\n",
    "1. Load pre-trained ResNet50 (trained on ImageNet)\n",
    "2. Freeze backbone layers (feature extractor)\n",
    "3. Replace final classification layer\n",
    "4. Train only the new classifier head\n",
    "5. Optional: Fine-tune later layers if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50802f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model architecture\n",
    "from models.resnet_model import create_resnet50\n",
    "\n",
    "# Create model\n",
    "num_classes = len(class_names)\n",
    "model = create_resnet50(num_classes=num_classes, pretrained=True)\n",
    "\n",
    "# Move to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"ðŸ—ï¸ Model Architecture: ResNet50\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"Trainable %: {trainable_params/total_params*100:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display model summary\n",
    "print(\"\\nðŸ“‹ Model Structure:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29754288",
   "metadata": {},
   "source": [
    "## 6. Training Configuration\n",
    "\n",
    "### 6.1 Loss Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function - CrossEntropyLoss for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer - Adam (adaptive learning rate)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate'],\n",
    "    weight_decay=1e-4  # L2 regularization\n",
    ")\n",
    "\n",
    "# Learning rate scheduler - ReduceLROnPlateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸ Training Configuration:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Loss function: CrossEntropyLoss\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Weight decay: 1e-4 (L2 regularization)\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau\")\n",
    "print(f\"  Factor: 0.5 (reduce by half)\")\n",
    "print(f\"  Patience: 5 epochs\")\n",
    "print(f\"Early stopping patience: {config['training']['early_stopping_patience']} epochs\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3cc09c",
   "metadata": {},
   "source": [
    "### 6.2 Training & Validation Functions\n",
    "\n",
    "Define core training and validation loops with metrics tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training', leave=False)\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validating', leave=False)\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Track metrics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"âœ… Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4027df",
   "metadata": {},
   "source": [
    "## 7. Model Training\n",
    "\n",
    "### 7.1 Training Loop with Early Stopping\n",
    "\n",
    "**Note**: Full training takes ~15 hours on CPU. This notebook demonstrates the training process with a small number of epochs. For full training, use the command-line script: `python src/models/run_training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e37cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for notebook (demo with fewer epochs)\n",
    "NUM_EPOCHS_DEMO = 3  # For demonstration - change to 50 for full training\n",
    "CHECKPOINT_DIR = Path.cwd().parent / 'results' / 'models'\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "EARLY_STOP_PATIENCE = config['training']['early_stopping_patience']\n",
    "\n",
    "print(f\"ðŸš€ Starting training for {NUM_EPOCHS_DEMO} epochs...\")\n",
    "print(f\"(For full training, use: python src/models/run_training.py)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350721ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for epoch in range(1, NUM_EPOCHS_DEMO + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS_DEMO}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc*100:.2f}%\")\n",
    "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        }, CHECKPOINT_DIR / 'best_model_demo.pth')\n",
    "        print(f\"âœ… New best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement ({patience_counter}/{EARLY_STOP_PATIENCE})\")\n",
    "        \n",
    "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"\\nâš ï¸ Early stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"âœ… Training complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db2b62",
   "metadata": {},
   "source": [
    "### 7.2 Training Curves Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', linewidth=2, marker='o')\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', linewidth=2, marker='s')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(epochs_range, [acc*100 for acc in history['train_acc']], \n",
    "             'b-', label='Train Acc', linewidth=2, marker='o')\n",
    "axes[1].plot(epochs_range, [acc*100 for acc in history['val_acc']], \n",
    "             'r-', label='Val Acc', linewidth=2, marker='s')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate plot\n",
    "axes[2].plot(epochs_range, history['learning_rates'], 'g-', linewidth=2, marker='^')\n",
    "axes[2].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Training Insights:\")\n",
    "print(f\"  Final train accuracy: {history['train_acc'][-1]*100:.2f}%\")\n",
    "print(f\"  Final validation accuracy: {history['val_acc'][-1]*100:.2f}%\")\n",
    "print(f\"  Best validation accuracy: {max(history['val_acc'])*100:.2f}%\")\n",
    "if history['val_acc'][-1] > history['train_acc'][-1]:\n",
    "    print(\"  âœ… Validation > Training suggests good generalization\")\n",
    "else:\n",
    "    print(\"  âš ï¸ Training > Validation may indicate overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e922730",
   "metadata": {},
   "source": [
    "## 8. Loading Pre-trained Model\n",
    "\n",
    "For evaluation and further analysis, load the best model from the full 50-epoch training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef630d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from full training\n",
    "best_model_path = Path.cwd().parent / 'results' / 'models' / 'best_model.pth'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(f\"ðŸ“¥ Loading best model from: {best_model_path}\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    \n",
    "    # Get model info from checkpoint\n",
    "    checkpoint_num_classes = checkpoint['model_state_dict']['model.fc.4.bias'].shape[0]\n",
    "    \n",
    "    # Create model with correct number of classes\n",
    "    eval_model = create_resnet50(num_classes=checkpoint_num_classes, pretrained=False)\n",
    "    eval_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    eval_model = eval_model.to(device)\n",
    "    eval_model.eval()\n",
    "    \n",
    "    print(f\"âœ… Model loaded successfully!\")\n",
    "    print(f\"   Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"   Validation Accuracy: {checkpoint.get('val_acc', 0)*100:.2f}%\")\n",
    "    print(f\"   Validation Loss: {checkpoint.get('val_loss', 0):.4f}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Best model not found at {best_model_path}\")\n",
    "    print(f\"   Using demo model for evaluation\")\n",
    "    eval_model = model\n",
    "    eval_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749e689",
   "metadata": {},
   "source": [
    "## 9. Quick Evaluation Check\n",
    "\n",
    "Let's do a quick accuracy check on the test set. For detailed analysis, see `03_results_analysis.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation on test set\n",
    "def quick_evaluate(model, dataloader, device):\n",
    "    \"\"\"Quick accuracy check\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc='Evaluating'):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (preds.cpu() == labels).sum().item()\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "# Run quick evaluation\n",
    "print(\"ðŸ” Quick Test Set Evaluation...\")\n",
    "test_accuracy = quick_evaluate(eval_model, test_loader, device)\n",
    "\n",
    "print(f\"\\nâœ… Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"\\nðŸ“Š For detailed analysis:\")\n",
    "print(f\"   - Confusion matrices\")\n",
    "print(f\"   - Per-class metrics\")\n",
    "print(f\"   - Error analysis\")\n",
    "print(f\"   - Confidence distributions\")\n",
    "print(f\"\\nâ†’ See notebook: 03_results_analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4414ca",
   "metadata": {},
   "source": [
    "## 10. Training Summary & Next Steps\n",
    "\n",
    "### What We Built\n",
    "\n",
    "**Model Configuration:**\n",
    "- âœ… ResNet50 with transfer learning\n",
    "- âœ… Custom classification head (16 classes)\n",
    "- âœ… Only ~4% parameters trainable (efficient)\n",
    "\n",
    "**Training Strategy:**\n",
    "- âœ… Adam optimizer with learning rate scheduling\n",
    "- âœ… Advanced data augmentation\n",
    "- âœ… Early stopping to prevent overfitting\n",
    "- âœ… Validation > Training accuracy (good generalization)\n",
    "\n",
    "**Key Insights:**\n",
    "1. Transfer learning dramatically speeds up convergence\n",
    "2. Data augmentation prevents overfitting effectively\n",
    "3. ReduceLROnPlateau helps escape local minima\n",
    "4. Model generalizes well to validation data\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**For Detailed Analysis** â†’ `03_results_analysis.ipynb`:\n",
    "- Comprehensive evaluation metrics\n",
    "- Confusion matrices and error analysis\n",
    "- Per-class performance breakdown\n",
    "- Prediction confidence analysis\n",
    "- Model interpretation and insights\n",
    "- Production readiness assessment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
